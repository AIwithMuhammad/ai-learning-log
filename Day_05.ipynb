{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1cd8e4d2",
      "metadata": {
        "id": "1cd8e4d2"
      },
      "source": [
        "\n",
        "# ðŸš— Day 05 â€” Model Selection & Hyperparameter Tuning\n",
        "\n",
        "**What you'll learn (with notes):**\n",
        "- Reuse Day 03/04 artifacts, keep transformations in **pipelines** (no leakage âœ…).\n",
        "- Compare multiple models fairly using **crossâ€‘validation**.\n",
        "- Tune hyperparameters with **RandomizedSearchCV**, **GridSearchCV**, and (bonus) **Optuna**.\n",
        "- Evaluate on a clean test set and interpret **feature importances** (permutation).\n",
        "\n",
        "> ðŸ“˜ Inspired by AurÃ©lien GÃ©ron, *Handsâ€‘On Machine Learning*, Ch. 2 (pipelines, CV) & Ch. 7 (model tuning).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5644d781",
      "metadata": {
        "id": "5644d781"
      },
      "source": [
        "## 0) Imports & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fdc4e903",
      "metadata": {
        "id": "fdc4e903"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from google.colab import files\n",
        "\n",
        "import joblib\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88ec66f7",
      "metadata": {
        "id": "88ec66f7"
      },
      "source": [
        "## 1) Load the cleaned dataset (from Day 03)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "763c339c",
      "metadata": {
        "id": "763c339c"
      },
      "outputs": [],
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "df = pd.read_csv(\"vehicles_cleaned.csv\")\n",
        "print(\"shape:\", df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5380cc1d",
      "metadata": {
        "id": "5380cc1d"
      },
      "source": [
        "\n",
        "## 2) Train/Test Split (holdâ€‘out first) + Target transform\n",
        "\n",
        "- Keep a **test set** aside to estimate realâ€‘world performance (GÃ©ron: avoid peeking!).  \n",
        "- Vehicle prices are often **rightâ€‘skewed** â†’ we model **log(price+1)** for stability, then convert back.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63e82b88",
      "metadata": {
        "id": "63e82b88"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Optional: create price buckets to stratify (comment out if it errors for tiny datasets)\n",
        "df = df.copy()\n",
        "df['price_cat'] = pd.cut(df['price'],\n",
        "                         bins=[0, 5000, 15000, 30000, 60000, np.inf],\n",
        "                         labels=[1,2,3,4,5])\n",
        "\n",
        "# Train/test split\n",
        "train_set, test_set = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE, stratify=df['price_cat'])\n",
        "\n",
        "for s in (train_set, test_set):\n",
        "    s.drop(columns=['price_cat'], inplace=True)\n",
        "\n",
        "# Target on log scale\n",
        "y_train = np.log1p(train_set['price'])\n",
        "X_train = train_set.drop(columns=['price'])\n",
        "\n",
        "y_test = np.log1p(test_set['price'])\n",
        "X_test  = test_set.drop(columns=['price'])\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c68118e",
      "metadata": {
        "id": "6c68118e"
      },
      "source": [
        "\n",
        "## 3) Light Feature Engineering (in code, not by hand)\n",
        "\n",
        "Keep transformations **inside the pipeline** where possible.  \n",
        "Here we only craft a simple, deterministic feature: **`car_age = current_year - year`**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9191363",
      "metadata": {
        "id": "e9191363"
      },
      "outputs": [],
      "source": [
        "\n",
        "current_year = datetime.datetime.now().year\n",
        "\n",
        "def add_basic_features(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    X = X.copy()\n",
        "    if 'year' in X.columns:\n",
        "        X['car_age'] = current_year - X['year']\n",
        "        X = X.drop(columns=['year'])\n",
        "    return X\n",
        "\n",
        "X_train = add_basic_features(X_train)\n",
        "X_test  = add_basic_features(X_test)\n",
        "\n",
        "X_train.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25e95e45",
      "metadata": {
        "id": "25e95e45"
      },
      "source": [
        "\n",
        "## 4) Preprocessing Pipelines (GÃ©ron Ch. 2)\n",
        "\n",
        "- **Numeric**: `SimpleImputer(median)` â†’ `StandardScaler`  \n",
        "- **Categorical**: `SimpleImputer(most_frequent)` â†’ `OneHotEncoder(handle_unknown=\"ignore\")`  \n",
        "- Combine via `ColumnTransformer` for one unified preprocessing step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9977815e",
      "metadata": {
        "id": "9977815e"
      },
      "outputs": [],
      "source": [
        "\n",
        "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "num_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "])\n",
        "\n",
        "cat_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer([\n",
        "    (\"num\", num_pipe, numeric_features),\n",
        "    (\"cat\", cat_pipe, categorical_features),\n",
        "])\n",
        "\n",
        "numeric_features, categorical_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4de2c23a",
      "metadata": {
        "id": "4de2c23a"
      },
      "source": [
        "### Helper: consistent crossâ€‘validation evaluation (on log target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "306d344c",
      "metadata": {
        "id": "306d344c"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def cv_rmse(model, X, y, cv=5):\n",
        "    scores = cross_val_score(model, X, y, scoring=\"neg_root_mean_squared_error\", cv=cv, n_jobs=-1)\n",
        "    return -scores.mean(), -scores.std()\n",
        "\n",
        "CV = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "CV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce309d46",
      "metadata": {
        "id": "ce309d46"
      },
      "source": [
        "## 5) Baseline model comparison (no tuning yet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34f57cf7",
      "metadata": {
        "id": "34f57cf7"
      },
      "outputs": [],
      "source": [
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "candidates = OrderedDict({\n",
        "    \"LinearRegression\": LinearRegression(),\n",
        "    \"DecisionTree\": DecisionTreeRegressor(random_state=RANDOM_STATE),\n",
        "    \"RandomForest\": RandomForestRegressor(random_state=RANDOM_STATE, n_estimators=200, n_jobs=-1),\n",
        "    \"HistGradientBoosting\": HistGradientBoostingRegressor(random_state=RANDOM_STATE, early_stopping=True),\n",
        "})\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, est in candidates.items():\n",
        "    pipe = Pipeline([\n",
        "        (\"prep\", preprocess),\n",
        "        (\"model\", est)\n",
        "    ])\n",
        "    mean_rmse, std_rmse = cv_rmse(pipe, X_train, y_train, cv=CV)\n",
        "    results.append({\"model\": name, \"cv_rmse_log\": mean_rmse, \"cv_rmse_log_std\": std_rmse})\n",
        "\n",
        "baseline_df = pd.DataFrame(results).sort_values(\"cv_rmse_log\")\n",
        "baseline_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b0e5688",
      "metadata": {
        "id": "4b0e5688"
      },
      "source": [
        "\n",
        "## 6) Hyperparameter Tuning (GÃ©ron Ch. 7)\n",
        "\n",
        "- Use **RandomizedSearchCV** for a broad, cheap scan.  \n",
        "- Refine with **GridSearchCV** on a narrower region if needed.  \n",
        "- Weâ€™ll tune **RandomForest** and **DecisionTree**. Weâ€™ll also try **HistGradientBoosting** (fast, strong baseline).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92cff83e",
      "metadata": {
        "id": "92cff83e"
      },
      "source": [
        "### 6.1 Decision Tree â€” GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6301d7ed",
      "metadata": {
        "id": "6301d7ed"
      },
      "outputs": [],
      "source": [
        "\n",
        "dt_pipe = Pipeline([\n",
        "    (\"prep\", preprocess),\n",
        "    (\"model\", DecisionTreeRegressor(random_state=RANDOM_STATE))\n",
        "])\n",
        "\n",
        "dt_param_grid = {\n",
        "    \"model__max_depth\": [None, 5, 10, 20, 30],\n",
        "    \"model__min_samples_split\": [2, 5, 10, 20],\n",
        "    \"model__min_samples_leaf\": [1, 2, 4, 8]\n",
        "}\n",
        "\n",
        "dt_grid = GridSearchCV(dt_pipe, dt_param_grid, cv=CV, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n",
        "dt_grid.fit(X_train, y_train)\n",
        "\n",
        "dt_best = dt_grid.best_estimator_\n",
        "dt_best_score = -dt_grid.best_score_\n",
        "\n",
        "print(\"DT best params:\", dt_grid.best_params_)\n",
        "print(\"DT CV RMSE (log):\", dt_best_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ca3947d",
      "metadata": {
        "id": "0ca3947d"
      },
      "source": [
        "### 6.2 Random Forest â€” RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc030cc5",
      "metadata": {
        "id": "dc030cc5"
      },
      "outputs": [],
      "source": [
        "\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "rf_pipe = Pipeline([\n",
        "    (\"prep\", preprocess),\n",
        "    (\"model\", RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1))\n",
        "])\n",
        "\n",
        "rf_param_dist = {\n",
        "    \"model__n_estimators\": randint(150, 500),\n",
        "    \"model__max_depth\": [None, 10, 20, 30],\n",
        "    \"model__min_samples_split\": randint(2, 20),\n",
        "    \"model__min_samples_leaf\": randint(1, 10),\n",
        "    \"model__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
        "}\n",
        "\n",
        "rf_rand = RandomizedSearchCV(\n",
        "    rf_pipe, rf_param_dist,\n",
        "    n_iter=30, cv=CV, random_state=RANDOM_STATE,\n",
        "    scoring=\"neg_root_mean_squared_error\", n_jobs=-1, verbose=0\n",
        ")\n",
        "rf_rand.fit(X_train, y_train)\n",
        "\n",
        "rf_best = rf_rand.best_estimator_\n",
        "rf_best_score = -rf_rand.best_score_\n",
        "\n",
        "print(\"RF best params:\", rf_rand.best_params_)\n",
        "print(\"RF CV RMSE (log):\", rf_best_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33ba5243",
      "metadata": {
        "id": "33ba5243"
      },
      "source": [
        "### 6.3 HistGradientBoosting â€” RandomizedSearchCV (fast, strong)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "390ed4f3",
      "metadata": {
        "id": "390ed4f3"
      },
      "outputs": [],
      "source": [
        "\n",
        "hgb_pipe = Pipeline([\n",
        "    (\"prep\", preprocess),\n",
        "    (\"model\", HistGradientBoostingRegressor(random_state=RANDOM_STATE, early_stopping=True))\n",
        "])\n",
        "\n",
        "hgb_param_dist = {\n",
        "    \"model__max_depth\": [None, 3, 5, 7, 10],\n",
        "    \"model__learning_rate\": uniform(0.01, 0.3),\n",
        "    \"model__l2_regularization\": uniform(0.0, 0.5),\n",
        "    \"model__max_iter\": randint(100, 500),\n",
        "    \"model__min_samples_leaf\": randint(10, 100),\n",
        "}\n",
        "\n",
        "hgb_rand = RandomizedSearchCV(\n",
        "    hgb_pipe, hgb_param_dist,\n",
        "    n_iter=30, cv=CV, random_state=RANDOM_STATE,\n",
        "    scoring=\"neg_root_mean_squared_error\", n_jobs=-1, verbose=0\n",
        ")\n",
        "hgb_rand.fit(X_train, y_train)\n",
        "\n",
        "hgb_best = hgb_rand.best_estimator_\n",
        "hgb_best_score = -hgb_rand.best_score_\n",
        "\n",
        "print(\"HGB best params:\", hgb_rand.best_params_)\n",
        "print(\"HGB CV RMSE (log):\", hgb_best_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad25e964",
      "metadata": {
        "id": "ad25e964"
      },
      "source": [
        "\n",
        "### 6.4 (Bonus) Optuna â€” smarter search\n",
        "\n",
        "You can comment this out if you want speed. This tunes **HistGradientBoosting** via crossâ€‘val.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9647831d",
      "metadata": {
        "id": "9647831d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# %%time\n",
        "try:\n",
        "    import optuna\n",
        "except Exception as e:\n",
        "    optuna = None\n",
        "    print(\"Optuna not found. Install with: pip install optuna\")\n",
        "\n",
        "if optuna is not None:\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            \"model__learning_rate\": trial.suggest_float(\"model__learning_rate\", 1e-3, 0.3, log=True),\n",
        "            \"model__max_depth\": trial.suggest_categorical(\"model__max_depth\", [None, 3, 5, 7, 10]),\n",
        "            \"model__l2_regularization\": trial.suggest_float(\"model__l2_regularization\", 0.0, 0.5),\n",
        "            \"model__max_iter\": trial.suggest_int(\"model__max_iter\", 100, 500),\n",
        "            \"model__min_samples_leaf\": trial.suggest_int(\"model__min_samples_leaf\", 10, 100),\n",
        "        }\n",
        "        pipe = Pipeline([(\"prep\", preprocess),\n",
        "                         (\"model\", HistGradientBoostingRegressor(random_state=RANDOM_STATE, early_stopping=True))])\n",
        "        pipe.set_params(**params)\n",
        "        scores = cross_val_score(pipe, X_train, y_train, scoring=\"neg_root_mean_squared_error\", cv=CV, n_jobs=-1)\n",
        "        return -scores.mean()\n",
        "\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=25, show_progress_bar=False)\n",
        "    print(\"Optuna best value (CV RMSE log):\", study.best_value)\n",
        "    print(\"Optuna best params:\", study.best_params)\n",
        "else:\n",
        "    print(\"Skipping Optuna section.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d00a7bc1",
      "metadata": {
        "id": "d00a7bc1"
      },
      "source": [
        "## 7) Pick the best tuned model (lowest CV RMSE on logâ€‘price)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b97675b9",
      "metadata": {
        "id": "b97675b9"
      },
      "outputs": [],
      "source": [
        "\n",
        "tuned = [\n",
        "    (\"DecisionTree\", dt_best, dt_best_score),\n",
        "    (\"RandomForest\", rf_best, rf_best_score),\n",
        "    (\"HistGradientBoosting\", hgb_best, hgb_best_score),\n",
        "]\n",
        "best_name, best_est, best_cv_rmse_log = sorted(tuned, key=lambda t: t[2])[0]\n",
        "print(\"Selected best model:\", best_name, \"| CV RMSE (log):\", best_cv_rmse_log)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "168ea571",
      "metadata": {
        "id": "168ea571"
      },
      "source": [
        "## 8) Final evaluation on the untouched test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1acd7793",
      "metadata": {
        "id": "1acd7793"
      },
      "outputs": [],
      "source": [
        "\n",
        "best_est.fit(X_train, y_train)\n",
        "\n",
        "y_pred_log = best_est.predict(X_test)\n",
        "test_rmse_log = mean_squared_error(y_test, y_pred_log, squared=False)\n",
        "\n",
        "# Transform back to original price units\n",
        "y_true = np.expm1(y_test)\n",
        "y_pred = np.expm1(y_pred_log)\n",
        "test_rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "test_r2   = r2_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Test RMSE (log scale): {test_rmse_log:.4f}\")\n",
        "print(f\"Test RMSE (price units): {test_rmse:,.2f}\")\n",
        "print(f\"Test R2 (price units): {test_r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da24c261",
      "metadata": {
        "id": "da24c261"
      },
      "source": [
        "## 9) Interpretability â€” Permutation Importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4398c9a8",
      "metadata": {
        "id": "4398c9a8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Get final feature names after ColumnTransformer\n",
        "def get_feature_names(preprocessor: ColumnTransformer):\n",
        "    output_features = []\n",
        "    # numeric\n",
        "    num_names = preprocessor.named_transformers_['num'].get_feature_names_out(numeric_features)                 if hasattr(preprocessor.named_transformers_['num'], 'get_feature_names_out') else numeric_features\n",
        "    output_features.extend(list(num_names))\n",
        "    # categorical\n",
        "    ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
        "    cat_names = ohe.get_feature_names_out(categorical_features)\n",
        "    output_features.extend(list(cat_names))\n",
        "    return output_features\n",
        "\n",
        "prep = best_est.named_steps['prep']\n",
        "feat_names = get_feature_names(prep)\n",
        "\n",
        "perm = permutation_importance(best_est, X_test, y_test, n_repeats=5, random_state=RANDOM_STATE, n_jobs=-1, scoring=\"neg_root_mean_squared_error\")\n",
        "imp_df = pd.DataFrame({\n",
        "    \"feature\": feat_names,\n",
        "    \"importance\": perm.importances_mean\n",
        "}).sort_values(\"importance\", ascending=False).head(25)\n",
        "\n",
        "imp_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5f607c3",
      "metadata": {
        "id": "d5f607c3"
      },
      "source": [
        "## 10) Save the best model + preprocess pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7affad6b",
      "metadata": {
        "id": "7affad6b"
      },
      "outputs": [],
      "source": [
        "\n",
        "out_dir = Path(\"./day05_artifacts\")\n",
        "out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "joblib.dump(best_est, out_dir / f\"best_model_{best_name}.joblib\")\n",
        "imp_df.to_csv(out_dir / \"permutation_importances_top25.csv\", index=False)\n",
        "\n",
        "print(\"Saved:\", list(out_dir.glob(\"*\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cd10299",
      "metadata": {
        "id": "9cd10299"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## âœ… What I learned\n",
        "- Pipelines prevent leakage and guarantee identical transforms for train/test.\n",
        "- Crossâ€‘validation gives a fairer estimate than a single split.\n",
        "- RandomizedSearch is efficient for broad scans; refine with GridSearch.\n",
        "- HistGradientBoosting is a great fast baseline.\n",
        "\n",
        "## ðŸ”œ Next\n",
        "- Try **GradientBoostingRegressor**, **XGBoost** or **LightGBM**.\n",
        "- Calibrate predictions or use **Quantile Regression** for price intervals.\n",
        "- Logâ€‘target vs. directâ€‘target comparison.\n",
        "- Export to **ONNX** / **PMML** or serve via **FastAPI** for deployment.\n",
        "\n",
        "Happy modeling! ðŸš€\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}