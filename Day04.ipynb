{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d65e734c",
      "metadata": {
        "id": "d65e734c"
      },
      "source": [
        "\n",
        "# ðŸš— Day 04 â€” Feature Engineering & Pipelines (Learning Notes)\n",
        "\n",
        "**Objective:**  \n",
        "Today we will take our cleaned `vehicles.csv` dataset (from Day 03) and:\n",
        "- Split into train/test sets\n",
        "- Engineer new features\n",
        "- Build preprocessing pipelines (numeric + categorical)\n",
        "- Train multiple baseline models: Random Forest, Linear Regression, Decision Tree\n",
        "- Compare their performance\n",
        "\n",
        "This follows the style from *Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow* (AurÃ©lien GÃ©ron, Chapter 2).  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fafcdc89",
      "metadata": {
        "id": "fafcdc89"
      },
      "source": [
        "## 1. Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d25b5d16",
      "metadata": {
        "id": "d25b5d16"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the cleaned dataset (output from Day 03)\n",
        "vehicles = pd.read_csv(\"/mnt/data/vehicles_cleaned.csv\")\n",
        "print(\"Shape:\", vehicles.shape)\n",
        "vehicles.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e09e9141",
      "metadata": {
        "id": "e09e9141"
      },
      "source": [
        "\n",
        "## 2. Train/Test Split\n",
        "\n",
        "As GÃ©ron emphasizes, always keep a test set aside before doing any heavy exploration or model training.  \n",
        "We'll stratify based on price categories to ensure balanced splits across cheap/expensive cars.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20140bc7",
      "metadata": {
        "id": "20140bc7"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Stratify based on price ranges\n",
        "vehicles[\"price_cat\"] = pd.cut(vehicles[\"price\"],\n",
        "                               bins=[0, 5000, 15000, 30000, 60000, np.inf],\n",
        "                               labels=[1,2,3,4,5])\n",
        "\n",
        "train_set, test_set = train_test_split(vehicles, test_size=0.2, stratify=vehicles[\"price_cat\"], random_state=42)\n",
        "\n",
        "print(\"Train size:\", len(train_set), \" Test size:\", len(test_set))\n",
        "\n",
        "# Drop the temporary stratification column\n",
        "for set_ in (train_set, test_set):\n",
        "    set_.drop(\"price_cat\", axis=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d296ef1",
      "metadata": {
        "id": "9d296ef1"
      },
      "source": [
        "## 3. Separate Features & Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eca601cd",
      "metadata": {
        "id": "eca601cd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Target variable\n",
        "y_train = train_set[\"price\"]\n",
        "X_train = train_set.drop(\"price\", axis=1)\n",
        "\n",
        "y_test = test_set[\"price\"]\n",
        "X_test = test_set.drop(\"price\", axis=1)\n",
        "\n",
        "X_train.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36315829",
      "metadata": {
        "id": "36315829"
      },
      "source": [
        "\n",
        "## 4. Feature Engineering\n",
        "\n",
        "We create new features:\n",
        "- **Car Age** = current year - year of the car\n",
        "- Optionally, **log-transform price** to reduce skewness (not done here yet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a362be6d",
      "metadata": {
        "id": "a362be6d"
      },
      "outputs": [],
      "source": [
        "\n",
        "import datetime\n",
        "\n",
        "current_year = datetime.datetime.now().year\n",
        "\n",
        "X_train = X_train.copy()\n",
        "X_test = X_test.copy()\n",
        "\n",
        "# Create new feature: car age\n",
        "X_train[\"car_age\"] = current_year - X_train[\"year\"]\n",
        "X_test[\"car_age\"] = current_year - X_test[\"year\"]\n",
        "\n",
        "# Drop original year column\n",
        "X_train.drop(\"year\", axis=1, inplace=True)\n",
        "X_test.drop(\"year\", axis=1, inplace=True)\n",
        "\n",
        "X_train.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0838c26e",
      "metadata": {
        "id": "0838c26e"
      },
      "source": [
        "\n",
        "## 5. Build Preprocessing Pipelines\n",
        "\n",
        "- Numeric Pipeline: Imputer â†’ StandardScaler  \n",
        "- Categorical Pipeline: Imputer â†’ OneHotEncoder  \n",
        "- Combine using ColumnTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56d1ad84",
      "metadata": {
        "id": "56d1ad84"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Identify numeric and categorical columns\n",
        "num_attribs = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_attribs = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "print(\"Numeric:\", num_attribs)\n",
        "print(\"Categorical:\", cat_attribs)\n",
        "\n",
        "# Numeric pipeline\n",
        "num_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "    ('scaler', StandardScaler()),\n",
        "])\n",
        "\n",
        "# Categorical pipeline\n",
        "cat_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
        "    ('onehot', OneHotEncoder(handle_unknown=\"ignore\")),\n",
        "])\n",
        "\n",
        "# Full preprocessing pipeline\n",
        "full_pipeline = ColumnTransformer([\n",
        "    (\"num\", num_pipeline, num_attribs),\n",
        "    (\"cat\", cat_pipeline, cat_attribs),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a3aa961",
      "metadata": {
        "id": "9a3aa961"
      },
      "source": [
        "## 6. Random Forest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8fae20f",
      "metadata": {
        "id": "b8fae20f"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "rf_pipeline = Pipeline([\n",
        "    (\"preprocess\", full_pipeline),\n",
        "    (\"model\", RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "rf_scores = cross_val_score(rf_pipeline, X_train, y_train,\n",
        "                            scoring=\"neg_mean_squared_error\", cv=5)\n",
        "rf_rmse = np.sqrt(-rf_scores)\n",
        "print(\"Random Forest CV RMSE:\", rf_rmse.mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7268d8b",
      "metadata": {
        "id": "b7268d8b"
      },
      "source": [
        "## 7. Linear Regression & Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81e78034",
      "metadata": {
        "id": "81e78034"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Linear Regression pipeline\n",
        "lr_pipeline = Pipeline([\n",
        "    (\"preprocess\", full_pipeline),\n",
        "    (\"model\", LinearRegression())\n",
        "])\n",
        "lr_scores = cross_val_score(lr_pipeline, X_train, y_train,\n",
        "                            scoring=\"neg_mean_squared_error\", cv=5)\n",
        "lr_rmse = np.sqrt(-lr_scores)\n",
        "print(\"Linear Regression CV RMSE:\", lr_rmse.mean())\n",
        "\n",
        "# Decision Tree pipeline\n",
        "dt_pipeline = Pipeline([\n",
        "    (\"preprocess\", full_pipeline),\n",
        "    (\"model\", DecisionTreeRegressor(random_state=42))\n",
        "])\n",
        "dt_scores = cross_val_score(dt_pipeline, X_train, y_train,\n",
        "                            scoring=\"neg_mean_squared_error\", cv=5)\n",
        "dt_rmse = np.sqrt(-dt_scores)\n",
        "print(\"Decision Tree CV RMSE:\", dt_rmse.mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0a81ae8",
      "metadata": {
        "id": "c0a81ae8"
      },
      "source": [
        "## 8. Evaluate on Test Set (Random Forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b27b9ff",
      "metadata": {
        "id": "1b27b9ff"
      },
      "outputs": [],
      "source": [
        "\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "final_preds = rf_pipeline.predict(X_test)\n",
        "final_rmse = mean_squared_error(y_test, final_preds, squared=False)\n",
        "print(\"Random Forest Test RMSE:\", final_rmse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "950cbdc4",
      "metadata": {
        "id": "950cbdc4"
      },
      "source": [
        "\n",
        "## 9. Next Steps & Learning Notes\n",
        "\n",
        "- Compare performances of different models (Linear Regression, Decision Tree, Random Forest)  \n",
        "- Try hyperparameter tuning (GridSearchCV / RandomizedSearchCV)  \n",
        "- Evaluate feature importance (Random Forest feature importances)  \n",
        "- Explore Gradient Boosting / XGBoost / LightGBM for better performance  \n",
        "\n",
        "This completes the full Day 04 notebook ðŸš€\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}